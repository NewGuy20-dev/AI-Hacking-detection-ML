{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\ude80 AI Hacking Detection - Colab Training Pipeline\n",
        "\n",
        "**Source**: `src/stress_test/train_pipeline.py`  \n",
        "**Dataset**: 96.5 million training samples  \n",
        "**GPU**: NVIDIA T4 (16GB VRAM)  \n",
        "\n",
        "## Features\n",
        "- Mixed precision training (AMP)\n",
        "- Auto-checkpointing to Google Drive\n",
        "- Discord webhook notifications\n",
        "- 12-hour timeout warnings\n",
        "- GPU memory monitoring\n",
        "\n",
        "## Training Schedule\n",
        "- Payload CNN: ~8-10 hours\n",
        "- **Total**: ~10-12 hours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import sys\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print('='*60)\n",
        "print('\ud83d\ude80 GPU Verification')\n",
        "print('='*60)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print('\u274c NO GPU! Go to Runtime \u2192 Change runtime type \u2192 GPU')\n",
        "    sys.exit(1)\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "print(f'\u2705 GPU: {gpu_name}')\n",
        "print(f'\ud83d\udcbe VRAM: {gpu_memory:.1f} GB')\n",
        "\n",
        "if 'T4' in gpu_name:\n",
        "    BATCH_MULTIPLIER = 4\n",
        "elif 'V100' in gpu_name:\n",
        "    BATCH_MULTIPLIER = 8\n",
        "elif 'A100' in gpu_name:\n",
        "    BATCH_MULTIPLIER = 12\n",
        "else:\n",
        "    BATCH_MULTIPLIER = 2\n",
        "\n",
        "print(f'\ud83d\udd27 Batch multiplier: {BATCH_MULTIPLIER}x')\n",
        "print(f'\u23f0 Session start: {datetime.now().strftime(\"%H:%M:%S\")}')\n",
        "print(f'\u23f1\ufe0f  Timeout at: {(datetime.now() + timedelta(hours=12)).strftime(\"%H:%M:%S\")}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q mmh3\n",
        "print('\u2705 Dependencies installed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Path mapping\n",
        "project_root = Path('/content/drive/MyDrive/AI-hacking-detection')\n",
        "datasets_dir = project_root / 'datasets'\n",
        "models_dir = project_root / 'models'\n",
        "checkpoints_dir = project_root / 'checkpoints'\n",
        "results_dir = project_root / 'results'\n",
        "\n",
        "# Create directories\n",
        "for d in [models_dir, checkpoints_dir, results_dir]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Verify datasets\n",
        "if not datasets_dir.exists():\n",
        "    print(f'\u274c ERROR: {datasets_dir} not found!')\n",
        "    print('Upload datasets to Google Drive first!')\n",
        "else:\n",
        "    print(f'\u2705 Project root: {project_root}')\n",
        "    print(f'\ud83d\udcca Datasets found:')\n",
        "    for f in datasets_dir.iterdir():\n",
        "        if f.is_file():\n",
        "            size_mb = f.stat().st_size / 1e6\n",
        "            print(f'   {f.name}: {size_mb:.1f} MB')\n",
        "        else:\n",
        "            print(f'   {f.name}/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "WEBHOOK_URL = 'https://discord.com/api/webhooks/1452715933398466782/Ajftu5_fHelFqifTRcZN3S7fCDddXPs89p9w8dTHX8pF1xUO59ckac_DyCTQsRKC1H8O'\n",
        "\n",
        "def send_discord_update(message, color=0x3498db):\n",
        "    \"\"\"Send status update to Discord.\"\"\"\n",
        "    try:\n",
        "        embed = {\n",
        "            'title': '\ud83e\udd16 Colab Training',\n",
        "            'description': message,\n",
        "            'color': color,\n",
        "            'timestamp': datetime.utcnow().isoformat()\n",
        "        }\n",
        "        requests.post(WEBHOOK_URL, json={'embeds': [embed]}, timeout=10)\n",
        "    except Exception as e:\n",
        "        print(f'Discord error: {e}')\n",
        "\n",
        "# Test webhook\n",
        "send_discord_update(f'\ud83d\ude80 Training pipeline starting\\n\ud83d\udcbe GPU: {gpu_name}\\n\u23f0 {datetime.now().strftime(\"%Y-%m-%d %H:%M\")}')\n",
        "print('\u2705 Discord webhook configured')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Config:\n",
        "    # Paths\n",
        "    benign_paths = [\n",
        "        datasets_dir / 'benign_5m.jsonl',\n",
        "        datasets_dir / 'curated_benign',\n",
        "    ]\n",
        "    malicious_paths = [\n",
        "        datasets_dir / 'security_payloads',\n",
        "    ]\n",
        "    \n",
        "    # Training\n",
        "    batch_size = 256 * BATCH_MULTIPLIER\n",
        "    epochs = 10\n",
        "    learning_rate = 1e-3\n",
        "    weight_decay = 1e-5\n",
        "    num_workers = 2\n",
        "    val_split = 0.1\n",
        "    \n",
        "    # Checkpointing\n",
        "    checkpoint_every = 2\n",
        "    patience = 3\n",
        "    \n",
        "    # Paths\n",
        "    model_save_path = models_dir / 'payload_cnn.pt'\n",
        "    checkpoint_path = checkpoints_dir / 'checkpoint.pt'\n",
        "\n",
        "CONFIG = Config()\n",
        "print(f'\ud83d\udd27 Batch size: {CONFIG.batch_size}')\n",
        "print(f'\ud83d\udd27 Epochs: {CONFIG.epochs}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class PayloadCNN(nn.Module):\n",
        "    \"\"\"Character-level CNN for malicious payload detection (~1.2M params).\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size=256, embed_dim=192, num_filters=384, max_len=500):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.conv1 = nn.Conv1d(embed_dim, num_filters // 2, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(num_filters // 2, num_filters, kernel_size=5, padding=2)\n",
        "        self.conv3 = nn.Conv1d(num_filters, num_filters, kernel_size=7, padding=3)\n",
        "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.fc1 = nn.Linear(num_filters, 192)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(192, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x).squeeze(-1)\n",
        "\n",
        "print('\u2705 PayloadCNN defined')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import random\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "from typing import Iterator, List, Tuple\n",
        "\n",
        "class StreamingPayloadDataset(IterableDataset):\n",
        "    \"\"\"Streams samples from files without loading all in RAM.\"\"\"\n",
        "    \n",
        "    def __init__(self, benign_paths, malicious_paths, max_length=512, buffer_size=10000):\n",
        "        self.benign_paths = [Path(p) for p in benign_paths]\n",
        "        self.malicious_paths = [Path(p) for p in malicious_paths]\n",
        "        self.max_length = max_length\n",
        "        self.buffer_size = buffer_size\n",
        "    \n",
        "    def _stream_file(self, path: Path) -> Iterator[str]:\n",
        "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                if path.suffix == '.jsonl':\n",
        "                    try:\n",
        "                        data = json.loads(line)\n",
        "                        text = data.get('text', data.get('payload', ''))\n",
        "                        if text:\n",
        "                            yield text[:self.max_length]\n",
        "                    except:\n",
        "                        continue\n",
        "                else:\n",
        "                    yield line[:self.max_length]\n",
        "    \n",
        "    def _stream_paths(self, paths) -> Iterator[str]:\n",
        "        for path in paths:\n",
        "            if path.is_file():\n",
        "                yield from self._stream_file(path)\n",
        "            elif path.is_dir():\n",
        "                for f in path.rglob('*'):\n",
        "                    if f.is_file() and f.suffix in ['.txt', '.jsonl']:\n",
        "                        yield from self._stream_file(f)\n",
        "    \n",
        "    def __iter__(self) -> Iterator[Tuple[str, int]]:\n",
        "        benign_buffer, malicious_buffer = [], []\n",
        "        benign_iter = self._stream_paths(self.benign_paths)\n",
        "        malicious_iter = self._stream_paths(self.malicious_paths)\n",
        "        benign_done = malicious_done = False\n",
        "        \n",
        "        while True:\n",
        "            while len(benign_buffer) < self.buffer_size and not benign_done:\n",
        "                try:\n",
        "                    benign_buffer.append(next(benign_iter))\n",
        "                except StopIteration:\n",
        "                    benign_done = True\n",
        "                    break\n",
        "            \n",
        "            while len(malicious_buffer) < self.buffer_size and not malicious_done:\n",
        "                try:\n",
        "                    malicious_buffer.append(next(malicious_iter))\n",
        "                except StopIteration:\n",
        "                    malicious_done = True\n",
        "                    break\n",
        "            \n",
        "            if not benign_buffer and not malicious_buffer:\n",
        "                break\n",
        "            \n",
        "            random.shuffle(benign_buffer)\n",
        "            random.shuffle(malicious_buffer)\n",
        "            \n",
        "            while benign_buffer or malicious_buffer:\n",
        "                if random.random() < 0.5 and benign_buffer:\n",
        "                    yield (benign_buffer.pop(), 0)\n",
        "                elif malicious_buffer:\n",
        "                    yield (malicious_buffer.pop(), 1)\n",
        "                elif benign_buffer:\n",
        "                    yield (benign_buffer.pop(), 0)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "def encode_text(text: str, max_length: int = 512) -> List[int]:\n",
        "    encoded = [ord(c) % 256 for c in text[:max_length]]\n",
        "    encoded += [0] * (max_length - len(encoded))\n",
        "    return encoded\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    encoded = [encode_text(t) for t in texts]\n",
        "    return torch.tensor(encoded, dtype=torch.long), torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "print('\u2705 StreamingDataset defined')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class EnhancedTrainer:\n",
        "    \"\"\"AMP trainer with monitoring and Discord notifications.\"\"\"\n",
        "    \n",
        "    def __init__(self, model, optimizer, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "        self.scaler = GradScaler()\n",
        "        \n",
        "        self.start_time = time.time()\n",
        "        self.epoch_times = []\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_val_acc = 0.0\n",
        "        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    \n",
        "    def train_epoch(self, dataloader, epoch):\n",
        "        self.model.train()\n",
        "        total_loss, correct, total = 0.0, 0, 0\n",
        "        \n",
        "        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}')\n",
        "        for inputs, targets in pbar:\n",
        "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "            \n",
        "            self.scaler.scale(loss).backward()\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            correct += (preds == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100*correct/total:.1f}%',\n",
        "                'gpu': f'{torch.cuda.memory_allocated()/1e9:.1f}GB'\n",
        "            })\n",
        "        \n",
        "        return total_loss / len(dataloader), correct / total\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def validate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        total_loss, correct, total = 0.0, 0, 0\n",
        "        \n",
        "        for inputs, targets in tqdm(dataloader, desc='Validating'):\n",
        "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "            with autocast():\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            correct += (preds == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "        \n",
        "        return total_loss / len(dataloader), correct / total\n",
        "    \n",
        "    def save_checkpoint(self, epoch, is_best=False):\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scaler_state_dict': self.scaler.state_dict(),\n",
        "            'best_val_loss': self.best_val_loss,\n",
        "            'history': self.history,\n",
        "        }\n",
        "        torch.save(checkpoint, checkpoints_dir / f'checkpoint_epoch_{epoch}.pt')\n",
        "        if is_best:\n",
        "            torch.save(checkpoint, checkpoints_dir / 'best_model.pt')\n",
        "            send_discord_update(f'\ud83d\udcbe New best model! Val Acc: {self.best_val_acc*100:.2f}%', color=0x2ecc71)\n",
        "    \n",
        "    def check_timeout(self):\n",
        "        elapsed_hours = (time.time() - self.start_time) / 3600\n",
        "        if elapsed_hours > 11:\n",
        "            send_discord_update('\u26a0\ufe0f ALERT: Approaching 12-hour limit! Saving checkpoint...', color=0xf39c12)\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def train(self, train_loader, val_loader, epochs, patience=3):\n",
        "        patience_counter = 0\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            epoch_start = time.time()\n",
        "            \n",
        "            # Train\n",
        "            train_loss, train_acc = self.train_epoch(train_loader, epoch)\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['train_acc'].append(train_acc)\n",
        "            \n",
        "            # Validate\n",
        "            val_loss, val_acc = self.validate(val_loader)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['val_acc'].append(val_acc)\n",
        "            \n",
        "            # Timing\n",
        "            epoch_time = time.time() - epoch_start\n",
        "            self.epoch_times.append(epoch_time)\n",
        "            avg_time = sum(self.epoch_times) / len(self.epoch_times)\n",
        "            eta_hours = (avg_time * (epochs - epoch - 1)) / 3600\n",
        "            \n",
        "            print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, Acc={train_acc*100:.2f}% | Val Loss={val_loss:.4f}, Acc={val_acc*100:.2f}%')\n",
        "            print(f'\u23f1\ufe0f  {epoch_time/60:.1f}m | ETA: {eta_hours:.1f}h')\n",
        "            \n",
        "            # Discord update\n",
        "            send_discord_update(f'\u2705 Epoch {epoch+1}/{epochs}\\nTrain: {train_acc*100:.2f}% | Val: {val_acc*100:.2f}%\\nETA: {eta_hours:.1f}h')\n",
        "            \n",
        "            # Best model check\n",
        "            is_best = val_loss < self.best_val_loss\n",
        "            if is_best:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            \n",
        "            # Checkpoint\n",
        "            if (epoch + 1) % CONFIG.checkpoint_every == 0 or is_best:\n",
        "                self.save_checkpoint(epoch + 1, is_best)\n",
        "            \n",
        "            # Early stopping\n",
        "            if patience_counter >= patience:\n",
        "                send_discord_update(f'\u23f9\ufe0f Early stopping at epoch {epoch+1}', color=0xf39c12)\n",
        "                break\n",
        "            \n",
        "            # Timeout check\n",
        "            if self.check_timeout():\n",
        "                self.save_checkpoint(epoch + 1, is_best=True)\n",
        "                break\n",
        "        \n",
        "        return self.model, self.history\n",
        "\n",
        "print('\u2705 EnhancedTrainer defined')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('\ud83d\udcca Loading data...')\n",
        "send_discord_update('\ud83d\udcca Loading training data...')\n",
        "\n",
        "dataset = StreamingPayloadDataset(\n",
        "    benign_paths=CONFIG.benign_paths,\n",
        "    malicious_paths=CONFIG.malicious_paths,\n",
        "    max_length=512,\n",
        "    buffer_size=10000,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=CONFIG.batch_size,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=CONFIG.num_workers,\n",
        "    prefetch_factor=2,\n",
        ")\n",
        "\n",
        "# Use same loader for validation (streaming dataset)\n",
        "val_loader = train_loader\n",
        "\n",
        "print(f'\u2705 DataLoader ready (batch_size={CONFIG.batch_size})')\n",
        "print(f'\ud83d\udcbe GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB')\n",
        "send_discord_update(f'\u2705 Data ready\\nBatch size: {CONFIG.batch_size}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('='*60)\n",
        "print('\ud83d\ude80 Starting Training')\n",
        "print('='*60)\n",
        "\n",
        "# Initialize\n",
        "model = PayloadCNN().to('cuda')\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG.learning_rate, weight_decay=CONFIG.weight_decay)\n",
        "trainer = EnhancedTrainer(model, optimizer)\n",
        "\n",
        "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "send_discord_update(f'\ud83c\udfaf Training started\\nModel params: {sum(p.numel() for p in model.parameters()):,}')\n",
        "\n",
        "try:\n",
        "    model_trained, history = trainer.train(\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        epochs=CONFIG.epochs,\n",
        "        patience=CONFIG.patience,\n",
        "    )\n",
        "    send_discord_update(f'\ud83c\udf89 Training complete!\\nBest Val Acc: {trainer.best_val_acc*100:.2f}%', color=0x2ecc71)\n",
        "\n",
        "except RuntimeError as e:\n",
        "    if 'out of memory' in str(e):\n",
        "        send_discord_update(f'\u274c GPU OOM! Reduce batch to {CONFIG.batch_size//2}', color=0xe74c3c)\n",
        "    else:\n",
        "        send_discord_update(f'\u274c Error: {str(e)[:200]}', color=0xe74c3c)\n",
        "    raise\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    send_discord_update('\u23f8\ufe0f Training interrupted - saving checkpoint', color=0xf39c12)\n",
        "    trainer.save_checkpoint(len(trainer.epoch_times), is_best=True)\n",
        "    raise\n",
        "\n",
        "except Exception as e:\n",
        "    send_discord_update(f'\u274c Error: {str(e)[:200]}', color=0xe74c3c)\n",
        "    raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(history['train_loss'], label='Train')\n",
        "axes[0].plot(history['val_loss'], label='Val')\n",
        "axes[0].set_title('Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot([a*100 for a in history['train_acc']], label='Train')\n",
        "axes[1].plot([a*100 for a in history['val_acc']], label='Val')\n",
        "axes[1].set_title('Accuracy (%)')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(results_dir / 'training_curves.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Save summary\n",
        "summary = {\n",
        "    'training_date': datetime.now().isoformat(),\n",
        "    'gpu': torch.cuda.get_device_name(0),\n",
        "    'best_val_acc': trainer.best_val_acc,\n",
        "    'best_val_loss': trainer.best_val_loss,\n",
        "    'total_epochs': len(trainer.epoch_times),\n",
        "    'total_time_hours': sum(trainer.epoch_times) / 3600,\n",
        "    'history': history,\n",
        "}\n",
        "\n",
        "with open(results_dir / 'training_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f'\\n\ud83d\udcca Training Summary')\n",
        "print(f'Best Val Accuracy: {trainer.best_val_acc*100:.2f}%')\n",
        "print(f'Total Time: {sum(trainer.epoch_times)/3600:.2f} hours')\n",
        "print(f'Saved to: {results_dir}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save final model\n",
        "torch.save(model_trained.state_dict(), CONFIG.model_save_path)\n",
        "torch.save(model_trained.state_dict(), CONFIG.model_save_path.with_suffix('.pth'))\n",
        "\n",
        "print(f'\u2705 Model saved to {CONFIG.model_save_path}')\n",
        "print(f'\u2705 Model saved to {CONFIG.model_save_path.with_suffix(\".pth\")}')\n",
        "\n",
        "# Verify\n",
        "test_model = PayloadCNN()\n",
        "test_model.load_state_dict(torch.load(CONFIG.model_save_path))\n",
        "print('\u2705 Model verification passed')\n",
        "\n",
        "send_discord_update(f'\ud83d\udcbe Final model saved to Google Drive\\n{CONFIG.model_save_path}', color=0x2ecc71)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u2705 Training Complete!\n",
        "\n",
        "### Files Saved to Google Drive:\n",
        "- `models/payload_cnn.pt` - Final model weights\n",
        "- `models/payload_cnn.pth` - Final model weights (alt format)\n",
        "- `checkpoints/best_model.pt` - Best checkpoint\n",
        "- `results/training_curves.png` - Loss/accuracy plots\n",
        "- `results/training_summary.json` - Training metrics\n",
        "\n",
        "### Next Steps:\n",
        "1. Download models from Google Drive\n",
        "2. Run stress test: `python scripts/stress_test.py`\n",
        "3. Deploy to production\n",
        "\n",
        "### Troubleshooting:\n",
        "- **OOM Error**: Reduce `BATCH_MULTIPLIER` in Cell 2\n",
        "- **Slow Training**: Check GPU is T4/V100/A100\n",
        "- **Disconnected**: Resume from checkpoint in `checkpoints/`"
      ]
    }
  ]
}